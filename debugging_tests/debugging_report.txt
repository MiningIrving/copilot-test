================================================================================
QWEN3 DEBUGGING ANALYSIS REPORT
================================================================================

IDENTIFIED ISSUES:
1. Tokenization failing - fundamental input processing issue
2. No token matches - complete output divergence from first step
3. Generation outputs completely different
4. Zero word overlap - outputs are completely unrelated
5. C++ implementation much slower than expected

DETAILED ANALYSIS:
- Tokenization working: False
- Token matches: 0/2
- Generation outputs match: False
- Average word overlap: 0.000
- C++/Python time ratio: 13364.40

ROOT CAUSE ANALYSIS:
- PRIMARY ISSUE: Tokenization failure indicates input processing problems
  * Check tokenizer configuration consistency
  * Verify vocabulary and special token handling

RECOMMENDED TESTING ORDER:
1. Test tokenizer configuration consistency
2. Verify input preprocessing steps
3. Check encoding/decoding pipelines
4. Test embedding layer with identical inputs
5. Verify model weight loading
6. Check first transformer layer computation
7. Test final layer normalization
8. Verify logits computation
9. Check sampling/generation logic
10. Run layer-by-layer output comparison
11. Test individual operators (RMSNorm, Attention, MLP)
12. Verify numerical precision settings

AVAILABLE DEBUGGING TESTS:
- basic_tensor_ops: 20 tests
- embedding_tests: 9 tests
- normalization_tests: 27 tests
- attention_tests: 9 tests
- mlp_tests: 9 tests
- config_tests: 2 tests

IMMEDIATE NEXT STEPS:
1. Build InfiniCore library to enable C++ operator testing
2. Run basic operator precision tests (RMSNorm, Linear, SwiGLU)
3. Use created test vectors to compare implementations
4. Focus on the identified primary issue area
5. Implement fixes incrementally and verify with layer tests
